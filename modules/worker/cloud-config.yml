#cloud-config

---
write_files:

    # Calico Version v2.6.2
    # https://docs.projectcalico.org/v2.6/releases#v2.6.2
    # This manifest includes the following component versions:
    #   calico/node:v2.6.2
    #   calico/cni:v1.11.0
    #   calico/kube-controllers:v1.0.0

    # This ConfigMap is used to configure a self-hosted Calico installation.
    kind: ConfigMap
    apiVersion: v1
    metadata:
      name: calico-config
      namespace: kube-system
    data:
      # Configure this with the location of your etcd cluster.
      etcd_endpoints: "http://127.0.0.1:2379"

      # Configure the Calico backend to use.
      calico_backend: "bird"

      # The CNI network configuration to install on each node.
      cni_network_config: |-
        {
            "name": "k8s-pod-network",
            "cniVersion": "0.1.0",
            "type": "calico",
            "etcd_endpoints": "__ETCD_ENDPOINTS__",
            "etcd_key_file": "__ETCD_KEY_FILE__",
            "etcd_cert_file": "__ETCD_CERT_FILE__",
            "etcd_ca_cert_file": "__ETCD_CA_CERT_FILE__",
            "log_level": "info",
            "mtu": 1500,
            "ipam": {
                "type": "calico-ipam"
            },
            "policy": {
                "type": "k8s",
                "k8s_api_root": "https://__KUBERNETES_SERVICE_HOST__:__KUBERNETES_SERVICE_PORT__",
                "k8s_auth_token": "__SERVICEACCOUNT_TOKEN__"
            },
            "kubernetes": {
                "kubeconfig": "__KUBECONFIG_FILEPATH__"
            }
        }

      # If you're using TLS enabled etcd uncomment the following.
      # You must also populate the Secret below with these files.
      etcd_ca: ""   # "/calico-secrets/etcd-ca"
      etcd_cert: "" # "/calico-secrets/etcd-cert"
      etcd_key: ""  # "/calico-secrets/etcd-key"

    ---

    # The following contains k8s Secrets for use with a TLS enabled etcd cluster.
    # For information on populating Secrets, see http://kubernetes.io/docs/user-guide/secrets/
    apiVersion: v1
    kind: Secret
    type: Opaque
    metadata:
      name: calico-etcd-secrets
      namespace: kube-system
    data:
      # Populate the following files with etcd TLS configuration if desired, but leave blank if
      # not using TLS for etcd.
      # This self-hosted install expects three files with the following names.  The values
      # should be base64 encoded strings of the entire contents of each file.
      # etcd-key: null
      # etcd-cert: null
      # etcd-ca: null

    ---

    # This manifest installs the calico/node container, as well
    # as the Calico CNI plugins and network config on
    # each master and worker node in a Kubernetes cluster.
    kind: DaemonSet
    apiVersion: extensions/v1beta1
    metadata:
      name: calico-node
      namespace: kube-system
      labels:
        k8s-app: calico-node
    spec:
      selector:
        matchLabels:
          k8s-app: calico-node
      template:
        metadata:
          labels:
            k8s-app: calico-node
          annotations:
            scheduler.alpha.kubernetes.io/critical-pod: ''
            scheduler.alpha.kubernetes.io/tolerations: |
              [{"key": "dedicated", "value": "master", "effect": "NoSchedule" },
               {"key":"CriticalAddonsOnly", "operator":"Exists"}]
        spec:
          hostNetwork: true
          serviceAccountName: calico-node
          # Minimize downtime during a rolling upgrade or deletion; tell Kubernetes to do a "force
          # deletion": https://kubernetes.io/docs/concepts/workloads/pods/pod/#termination-of-pods.
          terminationGracePeriodSeconds: 0
          containers:
            # Runs calico/node container on each Kubernetes node.  This
            # container programs network policy and routes on each
            # host.
            - name: calico-node
              image: quay.io/calico/node:v2.6.2
              env:
                # The location of the Calico etcd cluster.
                - name: ETCD_ENDPOINTS
                  valueFrom:
                    configMapKeyRef:
                      name: calico-config
                      key: etcd_endpoints
                # Choose the backend to use.
                - name: CALICO_NETWORKING_BACKEND
                  valueFrom:
                    configMapKeyRef:
                      name: calico-config
                      key: calico_backend
                # Cluster type to identify the deployment type
                - name: CLUSTER_TYPE
                  value: "k8s,bgp"
                # Disable file logging so `kubectl logs` works.
                - name: CALICO_DISABLE_FILE_LOGGING
                  value: "true"
                # Set Felix endpoint to host default action to ACCEPT.
                - name: FELIX_DEFAULTENDPOINTTOHOSTACTION
                  value: "ACCEPT"
                # Configure the IP Pool from which Pod IPs will be chosen.
                - name: CALICO_IPV4POOL_CIDR
                  value: "192.168.0.0/16"
                - name: CALICO_IPV4POOL_IPIP
                  value: "always"
                # Disable IPv6 on Kubernetes.
                - name: FELIX_IPV6SUPPORT
                  value: "false"
                # Set Felix logging to "info"
                - name: FELIX_LOGSEVERITYSCREEN
                  value: "info"
                # Set MTU for tunnel device used if ipip is enabled
                - name: FELIX_IPINIPMTU
                  value: "1440"
                # Location of the CA certificate for etcd.
                - name: ETCD_CA_CERT_FILE
                  valueFrom:
                    configMapKeyRef:
                      name: calico-config
                      key: etcd_ca
                # Location of the client key for etcd.
                - name: ETCD_KEY_FILE
                  valueFrom:
                    configMapKeyRef:
                      name: calico-config
                      key: etcd_key
                # Location of the client certificate for etcd.
                - name: ETCD_CERT_FILE
                  valueFrom:
                    configMapKeyRef:
                      name: calico-config
                      key: etcd_cert
                # Auto-detect the BGP IP address.
                - name: IP
                  value: ""
                - name: FELIX_HEALTHENABLED
                  value: "true"
              securityContext:
                privileged: true
              resources:
                requests:
                  cpu: 250m
              livenessProbe:
                httpGet:
                  path: /liveness
                  port: 9099
                periodSeconds: 10
                initialDelaySeconds: 10
                failureThreshold: 6
              readinessProbe:
                httpGet:
                  path: /readiness
                  port: 9099
                periodSeconds: 10
              volumeMounts:
                - mountPath: /lib/modules
                  name: lib-modules
                  readOnly: true
                - mountPath: /var/run/calico
                  name: var-run-calico
                  readOnly: false
                - mountPath: /calico-secrets
                  name: etcd-certs
            # This container installs the Calico CNI binaries
            # and CNI network config file on each node.
            - name: install-cni
              image: quay.io/calico/cni:v1.11.0
              command: ["/install-cni.sh"]
              env:
                # The location of the Calico etcd cluster.
                - name: ETCD_ENDPOINTS
                  valueFrom:
                    configMapKeyRef:
                      name: calico-config
                      key: etcd_endpoints
                # The CNI network config to install on each node.
                - name: CNI_NETWORK_CONFIG
                  valueFrom:
                    configMapKeyRef:
                      name: calico-config
                      key: cni_network_config
              volumeMounts:
                - mountPath: /host/opt/cni/bin
                  name: cni-bin-dir
                - mountPath: /host/etc/cni/net.d
                  name: cni-net-dir
                - mountPath: /calico-secrets
                  name: etcd-certs
          volumes:
            # Used by calico/node.
            - name: lib-modules
              hostPath:
                path: /lib/modules
            - name: var-run-calico
              hostPath:
                path: /var/run/calico
            # Used to install CNI.
            - name: cni-bin-dir
              hostPath:
                path: /opt/cni/bin
            - name: cni-net-dir
              hostPath:
                path: /etc/cni/net.d
            # Mount in the etcd TLS secrets.
            - name: etcd-certs
              secret:
                secretName: calico-etcd-secrets

---

# This manifest deploys the Calico Kubernetes controllers.
# See https://github.com/projectcalico/kube-controllers
apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  name: calico-kube-controllers
  namespace: kube-system
  labels:
    k8s-app: calico-kube-controllers
  annotations:
    scheduler.alpha.kubernetes.io/critical-pod: ''
    scheduler.alpha.kubernetes.io/tolerations: |
      [{"key": "dedicated", "value": "master", "effect": "NoSchedule" },
       {"key":"CriticalAddonsOnly", "operator":"Exists"}]
spec:
  # The controllers can only have a single active instance.
  replicas: 1
  strategy:
    type: Recreate
  template:
    metadata:
      name: calico-kube-controllers
      namespace: kube-system
      labels:
        k8s-app: calico-kube-controllers
    spec:
      # The controllers must run in the host network namespace so that
      # it isn't governed by policy that would prevent it from working.
      hostNetwork: true
      serviceAccountName: calico-kube-controllers
      containers:
        - name: calico-kube-controllers
          image: quay.io/calico/kube-controllers:v1.0.0
          env:
            # The location of the Calico etcd cluster.
            - name: ETCD_ENDPOINTS
              valueFrom:
                configMapKeyRef:
                  name: calico-config
                  key: etcd_endpoints
            # Location of the CA certificate for etcd.
            - name: ETCD_CA_CERT_FILE
              valueFrom:
                configMapKeyRef:
                  name: calico-config
                  key: etcd_ca
            # Location of the client key for etcd.
            - name: ETCD_KEY_FILE
              valueFrom:
                configMapKeyRef:
                  name: calico-config
                  key: etcd_key
            # Location of the client certificate for etcd.
            - name: ETCD_CERT_FILE
              valueFrom:
                configMapKeyRef:
                  name: calico-config
                  key: etcd_cert
          volumeMounts:
            # Mount in the etcd TLS secrets.
            - mountPath: /calico-secrets
              name: etcd-certs
      volumes:
        # Mount in the etcd TLS secrets.
        - name: etcd-certs
          secret:
            secretName: calico-etcd-secrets

---

# This deployment turns off the old "policy-controller". It should remain at 0 replicas, and then
# be removed entirely once the new kube-controllers deployment has been deployed above.
apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  name: calico-policy-controller
  namespace: kube-system
  labels:
    k8s-app: calico-policy
spec:
  # Turn this deployment off in favor of the kube-controllers deployment above.
  replicas: 0
  strategy:
    type: Recreate
  template:
    metadata:
      name: calico-policy-controller
      namespace: kube-system
      labels:
        k8s-app: calico-policy
    spec:
      hostNetwork: true
      serviceAccountName: calico-kube-controllers
      containers:
        - name: calico-policy-controller
          image: quay.io/calico/kube-controllers:v1.0.0
          env:
            # The location of the Calico etcd cluster.
            - name: ETCD_ENDPOINTS
              valueFrom:
                configMapKeyRef:
                  name: calico-config
                  key: etcd_endpoints

---

apiVersion: v1
kind: ServiceAccount
metadata:
  name: calico-kube-controllers
  namespace: kube-system

---

apiVersion: v1
kind: ServiceAccount
metadata:
  name: calico-node
  namespace: kube-system
coreos:

  locksmith:
    endpoint: https://etcd.${ internal-tld }:2379
    etcd_cafile: /etc/kubernetes/ssl/ca.pem
    etcd_certfile: /etc/kubernetes/ssl/k8s-etcd.pem
    etcd_keyfile: /etc/kubernetes/ssl/k8s-etcd-key.pem

  flannel:
    etcd_cafile: /etc/kubernetes/ssl/ca.pem
    etcd_certfile: /etc/kubernetes/ssl/k8s-worker.pem
    etcd_endpoints: https://etcd.${ internal-tld }:2379
    etcd_keyfile: /etc/kubernetes/ssl/k8s-worker-key.pem

  units:
    - name: etcd-member.service
      command: start
      drop-ins:
        - name: 01-wait-for-certs.conf
          content: |
            [Unit]
            After=create-certificates.service
            Requires=create-certificates.service
            ConditionFileNotEmpty=/etc/kubernetes/ssl/ca.pem
            ConditionFileNotEmpty=/etc/kubernetes/ssl/k8s-worker.pem
            ConditionFileNotEmpty=/etc/kubernetes/ssl/k8s-worker-key.pem

        - name: 10-environment.conf
          content: |
            [Service]
            Environment="ETCD_SSL_DIR=/etc/kubernetes/ssl"
            Environment="ETCD_CERT_FILE=/etc/ssl/certs/k8s-worker.pem"
            Environment="ETCD_CLIENT_CERT_AUTH=TRUE"
            Environment="ETCD_DISCOVERY_SRV=${ internal-tld }"
            Environment="ETCD_KEY_FILE=/etc/ssl/certs/k8s-worker-key.pem"
            Environment="ETCD_PEER_CERT_FILE=/etc/ssl/certs/k8s-worker.pem"
            Environment="ETCD_PEER_CLIENT_AUTH=true"
            Environment="ETCD_PEER_KEY_FILE=/etc/ssl/certs/k8s-worker-key.pem"
            Environment="ETCD_PEER_TRUSTED_CA_FILE=/etc/ssl/certs/ca.pem"
            Environment="ETCD_PROXY=on"
            Environment="ETCD_TRUSTED_CA_FILE=/etc/ssl/certs/ca.pem"

    - name: format-ephemeral.service
      command: start
      content: |
        [Unit]
        Description=Formats the ephemeral drive
        After=dev-xvdf.device
        Requires=dev-xvdf.device
        [Service]
        ExecStart=/usr/sbin/wipefs -f /dev/xvdf
        ExecStart=/usr/sbin/mkfs.ext4 -F /dev/xvdf
        RemainAfterExit=yes
        Type=oneshot

    - name: var-lib-docker.mount
      command: start
      content: |
        [Unit]
        Description=Mount ephemeral to /var/lib/docker
        Requires=format-ephemeral.service
        After=format-ephemeral.service
        Before=docker.service
        [Mount]
        What=/dev/xvdf
        Where=/var/lib/docker
        Type=ext4

    - name: download-cfssl.service
      command: start
      content: |
        [Unit]
        After=network-online.target
        Requires=network-online.target
        Before=etcd-member.service
        Description=Download cfssl
        [Service]
        Type=oneshot
        RemainAfterExit=yes
        ExecStartPre=-/usr/bin/mkdir --parents /etc/kubernetes/ssl
        ExecStartPre=-/usr/bin/mkdir --parents /opt/bin
        ExecStartPre=/usr/bin/curl -L -o /opt/bin/cfssl https://pkg.cfssl.org/R1.2/cfssl_linux-amd64
        ExecStartPre=/usr/bin/curl -L -o /opt/bin/cfssljson https://pkg.cfssl.org/R1.2/cfssljson_linux-amd64
        ExecStart=/usr/bin/chmod +x /opt/bin/cfssl /opt/bin/cfssljson

    - name: create-certificates.service
      command: start
      content: |
        [Unit]
        After=download-cfssl.service
        Requires=download-cfssl.service
        Before=flannel.service
        Description=Get ssl artifacts from s3 bucket using IAM role and create local certificates
        [Service]
        Type=oneshot
        RemainAfterExit=yes
        ExecStartPre=-/usr/bin/mkdir --parents /etc/kubernetes/ssl
        ExecStartPre=/opt/bin/fetch-from-s3 ca.pem
        ExecStart=/opt/bin/create-certificates

    - name: flanneld.service
      command: start
      drop-ins:
        - name: 50-network-config.conf
          content: |
            [Service]
            EnvironmentFile=-/etc/environment
            Environment="ETCD_SSL_DIR=/etc/kubernetes/ssl"
            Restart=always
            RestartSec=10

    - name: docker.service
      command: start
      drop-ins:
        - name: 40-flannel.conf
          content: |
            [Unit]
            After=flanneld.service
            Requires=flanneld.service
            [Service]
            Restart=always
            RestartSec=10

    - name: prefetch-rkt-hyperkube.service
      command: start
      content: |
        [Unit]
        After=network-online.target
        Requires=network-online.target
        Description=Prefetch rkt Hyperkube
        [Service]
        Type=oneshot
        RemainAfterExit=yes
        ExecStartPre=/usr/bin/rkt trust --trust-keys-from-https --prefix=quay.io/coreos/hyperkube
        ExecStart=/usr/bin/rkt fetch ${ hyperkube-image }:${ hyperkube-tag }

    - name: prefetch-docker-hyperkube.service
      command: start
      content: |
        [Unit]
        After=docker.service
        Requires=docker.service
        Description=Prefetch docker Hyperkube
        [Service]
        Type=oneshot
        RemainAfterExit=yes
        ExecStart=/usr/bin/docker pull ${ hyperkube-image }:${ hyperkube-tag }

    - name: kubelet.service
      command: start
      content: |
        [Unit]
        ConditionFileIsExecutable=/usr/lib/coreos/kubelet-wrapper
        ConditionFileNotEmpty=/etc/kubernetes/ssl/k8s-worker.pem
        ConditionFileNotEmpty=/etc/kubernetes/ssl/k8s-worker-key.pem
        After=flanneld.service
        After=prefetch-rkt-hyperkube.service
        After=prefetch-docker-hyperkube.service
        Requires=flanneld.service
        [Service]
        EnvironmentFile=/etc/environment
        Environment="KUBELET_ACI=${ hyperkube-image }"
        Environment="KUBELET_VERSION=${ hyperkube-tag }"
        Environment="RKT_OPTS=\
          --volume dns,kind=host,source=/etc/resolv.conf \
          --mount volume=dns,target=/etc/resolv.conf \
          --volume rkt,kind=host,source=/opt/bin/host-rkt \
          --mount volume=rkt,target=/usr/bin/rkt \
          --volume var-lib-rkt,kind=host,source=/var/lib/rkt \
          --mount volume=var-lib-rkt,target=/var/lib/rkt \
          --volume stage,kind=host,source=/tmp \
          --mount volume=stage,target=/tmp \
          --volume var-log,kind=host,source=/var/log \
          --mount volume=var-log,target=/var/log"
        ExecStartPre=/usr/bin/mkdir -p /var/log/containers
        ExecStartPre=/usr/bin/mkdir -p /var/lib/kubelet
        ExecStartPre=/usr/bin/mount --bind /var/lib/kubelet /var/lib/kubelet
        ExecStartPre=/usr/bin/mount --make-shared /var/lib/kubelet
        ExecStartPre=/usr/bin/systemctl is-active flanneld.service
        ExecStartPre=/opt/bin/wait-for-apiserver
        ExecStart=/usr/lib/coreos/kubelet-wrapper \
          --allow-privileged=true \
          --api-servers=https://master.${ internal-tld } \
          --cert-dir=/etc/kubernetes/ssl \
          --cloud-provider=aws \
          --cluster-dns=${ dns-service-ip } \
          --cluster-domain=${ cluster-domain } \
          --kubeconfig=/etc/kubernetes/kubeconfig.yml \
          --node-labels node-role.kubernetes.io/node \
          --pod-manifest-path=/etc/kubernetes/manifests \
          --register-node=true \
          --tls-cert-file=/etc/kubernetes/ssl/k8s-worker.pem \
          --tls-private-key-file=/etc/kubernetes/ssl/k8s-worker-key.pem
        Restart=always
        RestartSec=14
        [Install]
        WantedBy=multi-user.target

  update:
    reboot-strategy: etcd-lock

write-files:
  - path: /etc/environment
    permissions: 0644
    content: |
      COREOS_PRIVATE_IPV4=$private_ipv4
      ETCD_CA_FILE=/etc/kubernetes/ssl/ca.pem
      ETCD_CERT_FILE=/etc/kubernetes/ssl/k8s-worker.pem
      ETCD_KEY_FILE=/etc/kubernetes/ssl/k8s-worker-key.pem
      ETCDCTL_CA_FILE=/etc/kubernetes/ssl/ca.pem
      ETCDCTL_CERT_FILE=/etc/kubernetes/ssl/k8s-worker.pem
      ETCDCTL_KEY_FILE=/etc/kubernetes/ssl/k8s-worker-key.pem

  - path: /opt/bin/host-rkt
    permissions: 0755
    owner: root:root
    content: |
      #!/bin/sh
      exec nsenter -m -u -i -n -p -t 1 -- /usr/bin/rkt "$@"

  - path: /etc/kubernetes/kubeconfig.yml
    content: |
      apiVersion: v1
      kind: Config
      clusters:
        - name: local
          cluster:
            certificate-authority: /etc/kubernetes/ssl/ca.pem
            server: https://master.${ internal-tld }
      users:
        - name: kubelet
          user:
            client-certificate: /etc/kubernetes/ssl/k8s-worker.pem
            client-key: /etc/kubernetes/ssl/k8s-worker-key.pem
      contexts:
        - context:
            cluster: local
            user: kubelet
          name: kubelet-context
      current-context: kubelet-context

  - path: /etc/kubernetes/manifests/kube-proxy.yml
    content: |
      apiVersion: v1
      kind: Pod
      metadata:
        name: kube-proxy
        namespace: kube-system
      spec:
        hostNetwork: true
        containers:
        - name: kube-proxy
          image: ${ hyperkube-image }:${ hyperkube-tag }
          command:
          - /hyperkube
          - proxy
          - --kubeconfig=/etc/kubernetes/kubeconfig.yml
          - --master=https://master.${ internal-tld }
          securityContext:
            privileged: true
          volumeMounts:
            - mountPath: /etc/ssl/certs
              name: "ssl-certs"
            - mountPath: /etc/kubernetes/kubeconfig.yml
              name: "kubeconfig"
              readOnly: true
            - mountPath: /etc/kubernetes/ssl
              name: "etc-kube-ssl"
              readOnly: true
            - mountPath: /var/run/dbus
              name: dbus
              readOnly: false
        volumes:
          - name: "ssl-certs"
            hostPath:
              path: "/usr/share/ca-certificates"
          - name: "kubeconfig"
            hostPath:
              path: "/etc/kubernetes/kubeconfig.yml"
          - name: "etc-kube-ssl"
            hostPath:
              path: "/etc/kubernetes/ssl"
          - name: dbus
            hostPath:
              path: "/var/run/dbus"


  - path: /etc/logrotate.d/docker-containers
    content: |
      /var/lib/docker/containers/*/*.log {
        rotate 7
        daily
        compress
        size=1M
        missingok
        delaycompress
        copytruncate
      }

  - path: /opt/bin/fetch-from-s3
    permissions: 0755
    owner: root:root
    content: |
      #!/bin/bash -e
      until /usr/bin/rkt run \
        --net=host \
        --trust-keys-from-https \
        --volume=dns,kind=host,source=/etc/resolv.conf,readOnly=true --mount volume=dns,target=/etc/resolv.conf \
        --volume=ssl,kind=host,source=/etc/kubernetes/ssl,readOnly=false --mount=volume=ssl,target=/etc/kubernetes/ssl \
        quay.io/coreos/awscli -- aws s3 cp s3://${ s3-bucket }/$1 /etc/kubernetes/ssl
      do
        echo "retrying"
        sleep 5.2
      done
      echo "✓"

  - path: /opt/bin/wait-for-apiserver
    permissions: 0755
    owner: root:root
    content: |
      #!/bin/bash -e
      until curl --insecure https://master.${ internal-tld }/ &>/dev/null
      do
        echo "waiting for apiserver..."
        sleep 5.2
      done
      echo "✓"


  - path: /opt/bin/create-certificates
    permissions: 0755
    owner: root:root
    content: |
      #!/bin/bash -ex

      OUTDIR=/etc/kubernetes/ssl

      function error {
        echo "✗ Error on line $1"'!'
        exit 1
      }
      trap 'error $${LINENO}' ERR

      until printf "." && curl -d '{"label":"primary"}' http://pki.${ internal-tld }:8888/api/v1/cfssl/info &>/dev/null
      do sleep 5.2; done; echo "✓"


      DNS1="kubernetes"
      DNS2="kubernetes.default"
      DNS3="kubernetes.default.svc"
      DNS4="kubernetes.default.svc.cluster.local"
      DEFAULT_HOSTS="$DNS1,$DNS2,$DNS3,$DNS4,127.0.0.1"

      function csr {
        cat <<EOF
      {"CN":"$1","hosts":[""],"key":{"algo":"rsa","size":2048}}
      EOF
      }

      function generate {

        CN=$1
        PROFILE=$2
        HOSTS=$3

        echo "$(csr $CN)" \
          | /opt/bin/cfssl gencert \
            -remote=pki.${ internal-tld }:8888 \
            -profile=$PROFILE \
            -hostname="$HOSTS" - \
          | /opt/bin/cfssljson -bare $CN

        chmod 0644 $${CN}.pem $${CN}-key.pem

      }

      mkdir -p $OUTDIR && cd $OUTDIR

      generate k8s-worker client "$${DEFAULT_HOSTS},*.*.compute.internal,*.ec2.internal"

  - path: /etc/kubernetes/cni/net.d/10-flannel.conf
    content: |
        {
            "name": "podnet",
            "type": "flannel",
            "delegate": {
                "isDefaultGateway": true
            }
        }
